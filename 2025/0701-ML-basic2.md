## 지도(Supervision)

정답 있는 문제집 주고 가르치는거

### 지도학습(Supervised Learning)

정답이 있는 데이터를 가지고 모델이 정답을 맞추는 방법을 배우는 머신러닝 방식.


### 비지도학습

정답이 없는 데이터를 가지고 모델을 학습시키는 방법

**학습용 데이터셋 (train dataset)**
X (train) | Y (train) → 모델 학습

**평가용 데이터셋 (test dataset)**
X (test) | Y (test) → 모델 평가

---

### **1. 회귀 모델**

- **뭘 예측해?** `y` (정답)가 **연속형 숫자**일 때 (소수점까지 가능한 값).
- **예시:**
    - 집의 방 개수, 면적 등으로 **집 가격** (연속된 숫자) 예측.
    - 날씨 데이터로 내일의 **강수량** (연속된 숫자) 예측.
- **수학적 표현:** f:Rn→R (여러 입력으로 하나의 연속된 실수 값 예측)

### **2. 분류 모델**

- **뭘 예측해?** `y` (정답)가 **범주형(카테고리) 또는 이산형 숫자**일 때 (딱 떨어지는 값).
- **예시:**
    - 이메일 내용을 보고 **스팸메일인지 아닌지** (스팸/정상: 두 가지 카테고리) 구분.
- **수학적 표현:** f:Rn→{0,1,...,k} (여러 입력으로 정해진 카테고리 중 하나 예측)

---

**한 줄 요약:**

- **회귀 모델:** **숫자 예측** (예: 집값, 수익)
- **분류 모델:** **카테고리 구분** (예: 스팸 여부, 개/고양이)

 데이터의 종류 분류


      회귀 모델의 종류


**회귀 모델 방법론**

## **선형 회귀 모델(Linear Regression)**

데이터 점들을 가장 잘 통과하는 '최적의 직선'을 찾아 이후 데이터가 들어오면 어디쯤 있을지 예측할 때 쓰는 모델

이 직선은 y=ax+b라는 식으로 표현

여기서 a는 직선의 기울기 , b는 y축과 만나는 점(y절편)을 뜻한다.


MSE 손실함수


a와 b값을 계속 바꿔가며 직선 사이의 오차를 줄이는 방법 중 하나

실제 값y와 예측 값y햇의 차이를 구하고 그 차이를 제곱해서 계산하는 것

보통 제곱한 오차들을 다 더해서 평균을 냄

(모델의 예측이 얼마나 틀렸는지 숫자로 알려주는 중요한 지표)

수치적 해법
계속 반복해서 정답에 **수렴**하는 (가까워지는) 방식으로 답을 찾는 방법
조금씩 답에 가까워지면서 (점진적으로, 근사적으로) 해를 찾아가는 방법


해석적 해법
복잡한 계산 없이 **수학 공식을 이용해서 답을 한 번에  찾아내는 방법**
매우 빠르고, 간편하고, **정확한 답**을 얻을 수 있지만 공식이 알려진 경우만 사용 가능

최소제곱 선형회귀 분석


해석적 해법의 예시로 최소 제곱법으로 해석적 해법 풀이 한 것
**MSE(평균 제곱 오차) 손실 함수를 최소화하는 경우에 이 공식을 사용**

선형 회귀의 핵심 공식

- 풀이 설명
    - **X (대문자)**: 이건 **'입력 데이터'** 또는 '설명 변수'들을 모아 놓은 것
        - 이 X는 행렬이라는 형태
    - **y (소문자)**: 이건 **'실제 정답'** 또는 '예측하고 싶은 값'들을 모아 놓은 것
        - 이 y는 벡터라는 형태
    - **XT (엑스 트랜스포즈)**: 이건 X 행렬을 **'전치(Transpose)'** 시킨 것
        - 전치란, 행렬의 **행과 열을 서로 바꿔주는** 작업
        - 왜 바꾸냐? 뒤에서 다른 행렬이랑 곱하기 좋게 모양을 맞춰주는 작업
    - **(XTX)**: X를 전치시킨 XT와 원래 X를 **곱한** 것
        - 행렬끼리 곱하는 건 일반 숫자 곱셈이랑 달리 계산 규칙이 정해져 있다.
        - 이렇게 곱하면 새로운 행렬이 나오는데,
        - 이 행렬은 **데이터의 특징들이 압축되어 담겨 있는 정보 덩어리임.**
    - **(XTX)−1 (엑스 트랜스포즈 엑스 인버스)**: 방금 $(X^T X)$라는 행렬을 구했지? 이 행렬의 역행렬을 구한 것
        - 역행렬은 일반 숫자에서 '역수' 같은 개념.
        - 어떤 숫자 5가 있으면 역수는 1/5 이고, 둘을 곱하면 1
        - 행렬에서도 비슷한 역할. 어떤 행렬에 역행렬을 곱하면 '항등 행렬'이라는 특별한 행렬이 나오는데, 이건 마치 숫자 1과 같은 역할을 한다.
        - 쉽게 말해, 우리가 원하는 θ를 혼자 남기기 위해 필요한 **'나눗셈'과 비슷한 역할**. 행렬에서는 직접 나누는 개념이 없어 역행렬을 곱하는 방법을 사용
    - **XTy**: X의 전치 행렬(XT)과 y 벡터를 **곱한 것**
        - 이것도 데이터 간의 관계에 대한 중요한 정보를 담고 있는 결과물
    - **θ=(XTX)−1XTy**: 이제 최종적으로, $(X^T X)$의 역행렬과 $(X^T y)$를 **곱한** 게 바로 우리가 찾던 최적의 파라미터 θ

- **상관관계(Correlation)**
    
    한 변수(데이터)가 변할 때, **다른 변수도 같이 변하는 경향**을 보이는 관계
    
    **예)** 아이스크림 판매량이 늘면 에어컨 판매량도 늘어남
    
    그냥 '같이 움직이는구나' 하는 것. 한 변수가 다른 변수의 '원인'이라는 건 아님
    
- **인과관계(Causation)**
    
    한 변수의 변화가 **'원인'이 되어서** 그 결과로 **다른 변수를 변화시키는** 관계
    
    **예)** 에어컨을 켜면 방 온도가 낮아짐. (원인과 결과가 명확한 관계)
    

가장 중요한 차이점

인과관계가 있으면 반드시 상관관계도 있다.
하지만 **상관관계가 있다고 해서 무조건 인과관계가 있는 건 아님**

상관관계 판단은 쉽지만, 인과관계 판단은 훨씬 어렵다.

**로지스틱 회귀모델**

이'회귀'라는 이름이 붙어 있지만, 

사실 **두 가지 중 하나를 고르는 (이진 분류) 문제**를 풀 때 주로 쓰는 모델
**로지스틱 함수**를 써서 어떤 일이 일어날 **확률**을 계산할 수 있음
확장하면 **세 가지 이상으로 분류하는 (다중 분류) 문제**도 해결

## **상관관계 분석(Correlation Analysis)**

두 변수 사이의 상관관계를 판단하기 위한 분석과정

피어슨 상관계수

상관분석 결과로, 두 변수 사이에 어느 정도로 강한 선형상관관계가 있는지

-1 에서 1 범위의 숫자로 나타낸 것


상관관계 분석을 할 때 전제로 하는 4가지 기본 가정

하나라도 없으면 결과는 의미 없음

선형성 : 두 변수 X와 Y의 관계가 **직선 형태**여야 함


등분산성 : X의 값이 변하더라도 Y 값들의 흩어진 정도(분산)가 **일정해야** 함


정규성 : 각 변수가 모두 정규분포를 따라야 함

              일반적으로 분석 전 표준화 전처리 진행함

독립성 : 각 데이터 샘플이 **서로에게 영향을 주지 않고 독립적이어야** 함

## 상관행렬

데이터에 포함된 **모든 변수 쌍에 대한 상관계수**를 행렬(표) 형태로 나타낸 것
중복값과 대각선 원소 제거 후, 하삼각 행렬의 형태로 시각화함

데이터 탐색 시, 변수간의 상호 연관성 파악 유용함


### 공선성

한 변수를 알면 다른 변수를 거의 완벽하게 예측할 수 있는 관계

**두 변수 사이의 상관계수가 +1.0 또는 -1.0**일 때, 

이 두 변수 사이에 **공선성**이 있다고 표현

**+1.0**은 두 변수가 완벽하게 **같은 방향으로 움직인다**는 뜻
**-1.0**은 두 변수가 완벽하게 **반대 방향으로 움직인다**는 뜻 

### 다중공선성

**두 개 이상의 설명 변수들이 서로 강하게 상관되어 있을 때** 

발생하는 더 넓은 개념

공선성이 발생하면 나타나지 않을 때 까지 변수 제거하며 전처리함

## 상관계수는 직선의 기울기가 아니다

상관계수는 상관관계의 강도를 나타낸 것

회귀 직선 기울기 변해도 부호가 바뀌지 않는 한 값이 변하지 않음

반면 회귀분석의 결과로 나오는 회귀계수는

직선의 기울기 값 그 자체를 의미


분석에 사용하는 변수들이 모두 평균0 표준편차1인 표준정규분포를 따를땐 

상관계수와 회귀계수의 값이 일치하게 된다.

# 분류 모델의 정의

## 이진 분류

출력값이 참 혹은 거짓(0과1)로만 나오는 분류 모델

예) 질병검사 양성/음성

TMI : 로지스틱 회귀는 이진 분류문제를 위한 모델

결과값이 1일 확률을 예측하는 방식

### 확률 모델링에 로지스틱 함수가 필요한 이유

선형 회귀 모델의 한계 (확률 예측 시)

선형 회귀는 데이터를 가장 잘 나타내는 직선을 그림

문제는 확률은 0과 1 사이의 값만 가질 수 있다.

하지만 선형 회귀는 직선이기에 예측 결과가 0보다 작거나 1보다 커질 수 있음


이런 선형 회귀의 한계를 해결하기 위해 **로지스틱 함수**를 사용
이 S자 곡선은 어떤 입력 값이 들어오더라도 **출력값을 항상 0과 1 사이로 압축시킴**
이렇게 하면 모델의 출력값을 **확률로 자연스럽게 해석**할 수 있다.

### 로지스틱 함수

로지스틱 함수는 어떤 숫자를 넣어도 0과 1 사이의 값으로 바꿔주는 함수. 

주로 확률을 나타낼 때 사용. 

그래프는 S자 모양



### 로지스틱 회귀모델

일반 선형회귀 모델


로지스틱 함수 사용 모델


### 시그모이드 함수

시그모이드 함수는 S자 모양을 가진 함수들을 통틀어 부르는 이름
어떤 숫자(입력값)를 넣어도 항상 **0과 1 사이의 값으로 바꿔준다.**


하지만 딥러닝 에서 시그모이드 함수라고 하면 보통 로지스틱 함수를 말한다.

### 크로스엔트로피(Cross Entropy)

모델이 얼마나 잘 예측했는지 평가하는 '점수표' 

모델의 예측값(확률)과 실제 정답이 얼마나 차이 나는지 숫자로 보여주는 손실 함수


모델의 예측(확률)이 실제 정답과 얼마나 동떨어져 있는지 측정해서, 

모델이 학습하는 데 필요한 '오차'를 제공하는 중요한 도구이다.

로지스틱 회귀 모델처럼 0과1 예측하고 분류하는 문제에선 BCE가 MSE보다 훨씬 좋음

BCE  : 이진 크로스엔트로피

MSE : 평균 제곱 오차

BCE는 모델 학습 시 필요한 계산이(미분) 간단해져 더 효율적이다.


선형모델과 로지스틱 함수 , BCE 손실 함수를 미분에 넣었을때 결과가 초간단


# 다중 분류문제의 모델링

### 원 핫 인코딩(One Hot Encoding)

컴퓨터는 'Iris-Setosa' 같은 글자를 바로 이해 못 해. 

그래서 숫자로 바꿔줘야 하는데, 

그냥 0, 1, 2 이렇게 순서대로 바꾸면 컴퓨터가 2가 0보다 더 중요하다고 오해할 수 있음

그래서 **원 핫 인코딩은 '정답인 것만 1, 나머지는 다 0'으로 표시**하는 방법

- 예를 들어, 'Iris-Setosa'가 첫 번째 종류면 `[1, 0, 0]`
- 'Iris-Versicolour'가 두 번째 종류면 `[0, 1, 0]`
- 'Iris-Virginica'가 세 번째 종류면 `[0, 0, 1]`


### 소프트맥스(Softmax)

여러 개 중 하나를 고르는 문제(다중 분류)에서 쓰는 계산법

어떤 숫자들(logit 벡터)을 확률로 바꿔주는 함수. 확률은 0에서 1 사이 숫자고, 다 더하면 1이 돼


### 크로스엔트로피(Cross Entropy)

모델이 예측한 답이 얼마나 틀렸는지 알려주는 "손실 함수” 

1). Binary Cross Entropy (BCE)

답이 '예' 아니면 '아니오'처럼 딱 두 개일 때 쓰는 공식


2). **Cross Entropy (CE)** 

답이 여러 개일 때 쓰는 공식


### kNN(k-Nearest Neighbor) 알고리즘

새로운 데이터가 왔을때 주변 이웃을 보고 많이 속한 그룹으로 분류하는 방법

K값에 따라 분류가 달라짐


### 결정트리(의사결정나무, Decision Tree)

마치 스무고개처럼 질문을 하나씩 던지면서 정답을 찾아가는 방식의 예측 모델


불순도(Impurity)

한 범주 안에 데이터가 서로 얼마나 섞여 있는지를 측정하는 지표

결정트리의 학습과정에서 손실함수와 같은 역할을 함



## 랜덤포레스트와 모델의 앙상블

### 결정포레스트(Decision Forest or Random Forest)

여러 개의 결정 트리를 만들고, 내놓은 답들을 모아 최종 결론을 내는 것

### 모델 앙상블 (Ensemble)

앙상블은 **여러 개의 모델을 합쳐서 더 강력한 하나의 모델을 만드는 기술**을 말해. 랜덤 포레스트도 앙상블 기법 중 하나

**왜 앙상블을 쓸까?**

한 모델만으로는 완벽한 예측을 하기 어렵다. 

모델마다 잘하는 분야가 다르고, 틀리는 부분도 다르기 때문 

그래서 여러 모델의 장점을 합치고 단점을 보완해서 더 좋은 결과를 얻는다.

**앙상블의 종류**

**배깅 (Bagging)**

여러 모델을 **독립적으로** 만들어서 결과들을 모은다. (랜덤 포레스트가 여기에 속함) 

각 모델이 서로 간섭 없이 예측, 그 예측들을 평균 내거나 투표해서 최종 결정

**부스팅 (Boosting)**

모델들을 **순서대로** 만듦, 이전에 만든 모델이 틀렸던 부분을 다음 모델이 더 잘 맞추도록 학습. 

대표적으로 XGBoost, LightGBM 같은 것들이 있다.

## 서포트벡터머신(SVM)을 활용한 이진분류

### 결정경계(Decision Boundary)

이진 분류 모델에서 어떤 것을 기준으로 판단할지 정하는 선이나 면을 뜻함.

2차원은 선(직선), 3차원 이상은 초평면(Hyperplane)

### 선형 분리 가능 (Linearly Separable) 데이터셋

하나의 직선(또는 평면, 초평면)으로 두 클래스의 데이터를 

완벽하게 나눌 수 있는 데이터셋을 '선형 분리 가능하다'고 한다.


### 서포트 벡터 머신 (SVM, Support Vector Machine)

SVM은 두 종류 데이터를 나눌 때, **가장 멀리 떨어져서 경계선을 긋는 방법**

(이렇게 하면 새 데이터 와도 안정적으로 나눌 수 있음)


### 커널 트릭(Kernel Trick)

SVM은 **선형 분리 가능한** 데이터에만 적용 가능한데 실제 세상은 더 복잡함

커널 트릭은 이 문제를 해결하기 위한 방법

특정 커널 함수를 사용해 원래의 데이터를 **고차원 공간으로 변환**
저차원에서는 선형으로 분리하기 어려웠던 데이터가 고차원 공간으로 옮겨가면 

**선형 분리가 가능해지도록 만든다.**

이렇게 변환된 고차원 공간에서 SVM을 적용하여 최적의 결정 경계를 찾을 수 있다.
