## 1.다양한 문제의 정량평가 지표

**정확도의 한계 - Medical Test Paradox(겉으로 보이는 높은 정확도만으로 실제 상황을 오판할 수 있다)**

- 정확도 95%인 검사에서 양성이 나와도 실제 질병 확률은 약 16%
- 질병 유병률(1%)을 고려해야 함
- 단순 정확도만으로는 모델 성능을 제대로 평가하기 어려움


**평가지표 파악 요소**

- 최소화 지표 :값이 작을수록 좋은 지표 Minimizing Metric라고 부름
- 최대화 지표 :값이 클수록 좋은 지표  Maximizing Metric라고 부름

값의 범위와 최소/최대값 확인

예를 들어, 정확도는 보통 0%에서 100% 사이의 값을 가진다.(값의 범위)

## 2. 분류 문제 평가지표

**혼동 행렬 (Confusion Matrix)**

이진 분류 모델이 예측한 결과랑 실제 결과가 얼마나 맞고 틀렸는지를 표로 정리해 놓은 것

**표는 어떻게 보나?**

- **실제값 (Actual)**: 진짜 결과 (예: 실제로 병에 걸렸는지, 안 걸렸는지)
- **예측값 (Predicted)**: 모델이 예측한 결과 (예: 검사 양성/음성)

**네 가지 경우의 수**:

- **True Positive (TP)**: 모델이 양성이라고 **제대로(True)** 예측(실제도 양성)
- **True Negative (TN)**: 모델이 음성이라고 **제대로(True)** 예측(실제도 음성)
- **False Positive (FP)**: 모델이 양성이라고 예측했는데 **틀림(False)**. (**"1종 오류"** 또는 **"위양성"**)
- **False Negative (FN)**: 모델이 음성이라고 예측했는데 **틀림(False)**. (**"2종 오류"** 또는 **"위음성"**)

정확도만으로 모델 성능 평가 어려울때 사용

**주요 지표**

- **정밀도(Precision)**: TP/(TP+FP) - 양성 예측 중 실제 양성 비율
- **재현율(Recall)**: TP/(TP+FN) - 실제 양성 중 정확히 찾아낸 비율
- **F1 Score(H-mean)**: 정밀도와 재현율의 조화평균
- ROC : 모델이 '양성'인지 '음성'인지 판단하는 기준값을 계속 바꿔가며 TP,FP변화 그래프로 보여줌
- **AUC(Area Under ROC Curve)**: ROC 곡선 아래 면적/**넓이**를 측정한 값

## 3. 회귀 문제 평가지표

- **MAE(Mean Absolute Error)**: 오차 절대값의 평균
- **MSE(Mean Squared Error)**: 오차 제곱의 평균
- **RMSE(Root Mean Squared Error)**: MSE의 제곱근

**특성 차이**

- MAE: 오차가 0인지 여부에 민감
- MSE: 이상치(Outlier)에 더 민감하게 반응

## 4. 기타 평가지표

**성능 관련**

- 편집거리(Edit Distance): 시계열 모델에서 사용
- MOS(Mean Opinion Score): 생성모델 평가

**효율성 관련**

- 인퍼런스 시간(Latency): 모델이 샘플 한개, 또는 여러개를 한번에 처리하는데에 걸리는 시간
- 처리율(Throughput): 단위시간당 처리 샘플 수
- 메모리 사용량(Memory Usage): GPU 메모리 점유량

## 5. 평가지표 선택 방법

**선택 기준**

- 연구 목적: 기존 연구의 측정방식 참고
- 서비스 목적: 서비스 목표에 직접 연결된 지표 선택

**데이터 불균형 문제**

- 일반적: 정확도 사용
- 클래스 불균형: 정밀도, 재현율, F1 Score 추가 사용

- **MAE**: 오차가 **0에 가까운지**가 중요할 때 좋고, 이상치에는 상대적으로 덜 민감함.
- **MSE**: 모델에 **이상치(큰 오차)가 있는지 없는지**를 파악하는 데 더 유용
- 이상치에 큰 페널티를 주기 때문에 모델이 이상치에 영향을 받지 않도록 훈련시킬 때 많이 사용함.

## 6. 복수 지표 활용

**방법**

1. 여러 기준 같이 보기. 서로 다른 지표 동시 사용 (MSE + MAE) 
2. 데이터 나눠서 보기. 여러 데이터셋에서 같은 지표 계산
3. 중요한거 점수 더 주기.  가중평균으로 최종 평가 (예: 0.3×A + 0.7×B)

**우선순위 설정**

- **최적화 지표(Optimizing Metric)**: 모델 선택에 가장 중요한 지표
- **충족 지표(Satisficing Metric)**: 임계값만 넘으면 되는 지표

**예시**: 정확도(최적화) vs GPU메모리(충족) - 메모리 한계 내에서 정확도 최고인 모델 선택


---

## 베이스라인(Baseline) 스코어

랜덤 베이스라인은 내가 학습중인 모델과 비교용인것 정확도는 아무거나 찍어서 맞출 확률

예) 상자5개중 원하는거 맞출 확률=20퍼(랜덤 베이스라인 정확도)

평가지표 측정용으로 사용

이거보다 못하면 문제 있는거라 확인할 것

### 다른 방식의 베이스라인

랜덤찍기는 쪼까 그럴 때 쓰는 베이스라인 스코어

나이브 베이즈 분류기 : 특징들 따로 논다고 가정하고, 확률로 분류 
k-최근접이웃 분류      : 새로운게 오면 가장 가까운 애들 따라 분류
다중 로지스틱 회귀     : 각 분류에 속할 확률 계산해서 분류

### 리더보드 (Leaderboard)

연구자들이 각자 학습한 모델의 성능을 **공유하고 비교하기 위한 순위표**

### SOTA(State Of The Art)

'최첨단'이라는 뜻으로 쓰이며, 리더보드에서는 **1위를 달성한, 세계 최고 성능의 모델**을 뜻함

## 베이즈 최적 에러(Bayes Optimal Error)

현재의 정량 평가 방식에서 **이론상 도달할 수 있는 이상적인 분류 체계의 성능**을 의미
아무리 뛰어난 모델을 만들더라도 이 에러율 이하로는 낮출 수 없다는 이론적 한계치

휴먼 퍼포먼스(Human performance)

사람이 어떤 일을 얼마나 잘하는지 측정하는 것(사람이 할 수 있는 최고 수준의 성능)


---

### 정량평가

데이터를 숫자로 딱 떨어지게 모델을 평가하는 거

모델 학습이나 선택에 편하고 자동화하기도 좋음

### 정성평가

숫자로 나타내기 어려운, 좀 더 주관적이고 깊이 있는 평가

예) 감상, 창의력,아이디어,경험 같은것

이 둘은 상호보완성

정량평가만으로는 모델이 왜 틀렸는지 알 수 없다.

정량적인 수치만으로는 모델의 실제 성능이나 발생하는 미묘한 오류를 전부 파악하기 어렵기 때문. 특히 사람의 주관적인 판단이 필요하면 정성평가가 필요 

모델이 복잡해서 내부를 직접 보기 어려우니, 

모델이 뭘 입력받아서 어떤 결과를 내는지 비교하면서 정성적으로 평가해야 한다.

**데이터셋의 정성평가**랑 **모델의 정성평가** 가 비슷하게 진행된다는 뜻

## 체계적인 정성평가 방법론

정성평가가 어려운 이유

1.사람이 직접 해서 힘들고 데이터 많아지면 더 힘듬 중요하지만 안하게됨

2.사람의 주관이 들어가기 쉬워서 공정하고 일관되게 하기가 어렵다

정성평가 팁

시간을 대폭 줄여주진 않지만 몇 번씩 반복하는 과정은 막을 수 있음

1. 정성평가를 시작하기 전에, 소수의 전문가가 미리 해보고 속도와 비효율적인 부분을 분석해서 개선 계획을 세운다.

1. 무리하지 않게 소규모로 먼저 계획하고, 샘플은 무작위로 뽑아서 확인 한다.

1. 정성평가는 점진적으로 진행해서 효율성을 높여야 한다
- **소규모 분석으로도 큰 문제(쉬운 버그)를 발견해서 전체 틀린 샘플의 상당수를 미리 걸러낼 수 있고,**
- **그렇게 개선된 모델로 다시 틀린 샘플을 찾아보면 훨씬 효율적으로 정성평가를 할 수 있기 때문**

1. 단순히 '틀렸다'가 아니라 **'왜 틀렸는지'를 명확하게 기록하고, 비슷한 문제끼리 묶어서 어떤 것부터 고칠지 계획을 세워야함**

1. 모델의 성능에 문제를 일으키는 인풋 데이터의 패턴을 트리거 라고 하는데,
트리거 샘플들을 모아 별도의 평가용 데이터셋을 구성하면,
**정량평가만으로도 해당 이슈의 추적이 가능**해진다

---

### 모델의 복잡도(Complexity, Capacity)

어려운 데이터까지 학습할 수 있는 능력을 말함

파라미터 수나 층, 뉴런 수가 많을수록 복잡해지고 이걸 용량 이라고 부름

**모델의 복잡도가 높다고 무조건 좋은 건 아니다**


### 과적합(Overfitting)

모델이 학습 데이터의 패턴에 너무 완벽하게 맞춰져서(암기해서), 

실제 처음 보는(새로운) 데이터에는 성능이 안 좋아지는 현상

### 과소적합(Underfitting)

모델이 너무 단순해서 학습 데이터조차 제대로 배우지 못해, 

학습 데이터에서도 성능이 안 좋은 상태를 말함


### 적합도 균형의 조절

핵심은 모델의 복잡도와 데이터의 복잡성(품질, 다양성) 사이의 균형을 맞추는 것

- **과적합일 때 (모델 복잡도 > 데이터 복잡성):**
    - 모델 복잡도를 줄이거나 학습 데이터를 늘려야 함
- **과소적합일 때 (모델 복잡도 < 데이터 복잡성):**
    - 모델 복잡도를 늘리거나 학습 데이터를 줄여야 함


(모델이 너무 작아서 데이터를 다 못 담아내는 과소적합 상황)

### 편향, 분석과 에러 분석

모델의 오류는 주로 편향(과소적합)이나 분산(과적합) 때문에 발생

에러 분석은 모델이 왜 틀리는지(편향 때문인지, 분산 때문인지) 진단해서 

효율적으로 개선하기 위한 방법


과소적합은 모델이 너무 단순해서 생기는 '큰 편향' 문제고, 

과적합은 모델이 너무 복잡해서 생기는 '큰 분산' 문제


모델의 성능을 학습(Train), 검증(Valid), 테스트(Test) 데이터셋에서 비교해서 

모델이 '큰 편향(과소적합)' 때문인지, 아니면 '큰 분산(과적합)' 때문인지 진단할 수 있다.


학습 데이터는 엄청 잘 맞추는데(예: 99%), 

실제 처음 보는 데이터(검증이나 테스트 데이터)에서는 갑자기 못 맞춰(예: 78%) 버리는 상황.

이게 바로 과적합 때문이다

이유

1. 학습 데이터를 '암기'해버려서
2. 새로운 상황에 '일반화'를 못해서

과소적합이면 **학습(Train)이랑 검증(Valid)의 수치 둘 다  낮게 나온다.**

## 과적화 문제의 해결방법

### L1 정규화 (Lasso Regularization)

모델의 오차에 각 가중치들의 **절댓값**을 더해서 벌칙을 주는 방식
가중치를 0으로 만들어서 **특징을 선택**하는 데 유리하고, 모델을 더 **간단하게** 만들어줌

### L2 정규화 (Ridge Regularization)

모델의 오차에 각 가중치들의 **제곱값**을 더해서 벌칙을 주는 방식
가중치를 0에 가깝게 만들지만 완전히 0으로 만들지 않아서 

모든 특징의 영향을 **고르게 줄여주고**, 모델의 **일반화 성능**을 높이는 데 효과적이다.

둘 다 과적합을 막아서 모델의 일반화 성능을 높이는 게 궁극적인 목표

### 학습 데이터의 증강(Augmentation)

있는 데이터를 살짝 변형해 새 데이터처럼 만들어 학습량을 늘리는 것
이렇게 하면 모델이 여러 상황을 보고 배워서, **과적합을 막고 실제 상황에서도 잘 맞춘다.**

### 학습곡선(Learning Curve)

학습 곡선과 그래프는 모델이 학습하면서 **성능이 어떻게 변하는지 보여주는 그림**


### 조기 종료(Early Stopping)

모델을 학습시킬 때, **새로운 데이터를 잘 못 맞추기 시작하면 (과적합되려고 할 때)** 

**바로 학습을 멈추는 것**

이렇게 하면 모델이 쓸데없이 너무 똑똑해져서 망가지는 걸 막고, 

**가장 성능이 좋을 때 학습을 끝낼 수 있어**

### 편향과 분산의 Trade Off 관계

편향 : 모델이 너무 단순해서 **핵심 패턴을 놓치는 것**

분산 : 모델이 너무 복잡해서 **쓸데없는 것까지 다 외워버리는 것**

이 둘은 **시소처럼 한쪽이 낮아지면 다른 한쪽이 높아진다.**

편향이 높으면 과소적합

분산이 높으면 과적합

그래서 우리는 **딱 중간 지점**, 최소 에러가 나오는 곳을 찾아야 한다.

(모델의 성능이 가장 좋은 지점)

## 머신러닝 모델의 Double Descent 학습곡선

더블 디센트는 모델을 너무 똑똑하게 만들어도 다시 좋아질 수 있다.

원래는 모델이 너무 복잡해지면 과적합돼서 성능이 나빠진다고 했음

그치만 **더블 디센트**는 모델의 복잡도를 **아주 아주 많이 높이면** 

(예전엔 생각도 못 했던 엄청나게 큰 모델들)

오히려 다시 **새로운 데이터에 대한 성능(Valid Loss)이 좋아지는 현상**

**아주 큰 모델이나 아주 많은 데이터를 쓸 때 나타나는 현상**으로,

기존의 과적합 개념을 넘어서는 새로운 발견
