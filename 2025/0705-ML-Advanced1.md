### 정확도  (Accuracy)

전체 데이터 중에서 모델이 

**얼마나 많은 데이터를 올바르게 예측했는지**를 나타내는 비율


혼동 행렬은 모델이 예측한 결과가 실제 정답과 얼마나 일치하는지 보여주는 **표**

혼동 행렬에서 TP와 TN의 합을 전체(TP+FP+FN+TN)로 나눈 값이 정확도

### 정밀도  (Precision)

모델이 긍정이라고 예측한 것들 중에서 

**실제로 긍정인 것이 얼마나 되는지** 나타내는 비율


TP / (TP + FP)

### 재현율 ( Recall ) = 민감도 ( Sensitivity )

실제로 긍정인 데이터들 중에서 모델이 ****

**얼마나 많은 긍정을 정확히 찾아냈는지**를 나타내는 비율


 TP / (TP + FN)

### 특이도 ( Specificity )

실제로 부정 인 데이터들 중에서 모델이 ****

**얼마나 많은 '부정'을 정확히 '부정'이라고 예측했는지**를 나타내는 비율


TN / (TN + FP)

### F1 score

정밀도와 재현율의 조화평균

데이터가 한쪽으로 치우쳐져 있을 때 

정확도만으로는 모델 성능을 제대로 평가하기 어려울 때 유용하게 쓰이는 지표


### TPR과 FPR

● TPR (True Positive Rate): 진짜 중에서 진짜로 예측한 비율
● FPR (False Positive Rate): 가짜 중에서 진짜로 예측한 비율

### ROC 곡선 ( ROC curve )

모델의 성능을 여러 기준점에서 한눈에 보여주는 그래프

 FPR과 TPR을 각각 x,y축으로 가지는 그래프

좌상단에 치우칠수록 높은 성능  

= TPR은 높고 FPR은 낮을수록 모델의 성능이 더 좋다는 의미


### AUC (Area Under the ROC Curve)

ROC 곡선 아래의 면적
이 값은 모델의 성능을 **하나의 수치로 요약해서 보여주는 기준**

1에 가까울수록 그래프가 좌상단에 근접하므로 좋은 성능


### IoU (Intersection over Union)

**객체 탐지** 모델의 성능을 평가하는 방법 중 하나

모델이 예측한 바운딩 박스(초록색)와 실제 정답 바운딩 박스(빨간색)가 

**얼마나 잘 겹치는지**를 수치로 나타냄 1에 가까울수록 가장 높은 점수



threshold를 기준으로

IoU ≥ threshold일 경우 예측이 맞았다고 구분
IoU < threshold일 경우 예측이 틀렸다고 구분

